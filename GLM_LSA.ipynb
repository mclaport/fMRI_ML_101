{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta-series modelling - Extracting LSA\n",
    "Based on: https://nilearn.github.io/dev/auto_examples/07_advanced/plot_beta_series.html\n",
    "Adapted by: Maria Clara Laport and Tiago Bortolini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, fnmatch, glob\n",
    "from nilearn import image\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.reporting import make_glm_report\n",
    "from nilearn.image import mean_img, math_img\n",
    "from nilearn.masking import apply_mask\n",
    "import nibabel as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Setting Model Parameters\n",
    "Setting the fixed parameters that will be used in the three types of modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fixed (GLM) parameters\n",
    "\n",
    "# Fmriprep space\n",
    "space = 'MNI152NLin2009cAsym'\n",
    "\n",
    "hrf_model='spm'\n",
    "t_r = 1.0  \n",
    "\n",
    "slice_time_ref=0.5\n",
    "\n",
    "drift_model = 'cosine' # Specifies the desired drift model. Default=’cosine\n",
    "\n",
    "high_pass = 0.0078125 # High-pass frequency in case of a cosine model (in Hz). Default=0.01.\n",
    "drift_order = 1 #Order of the drift model (in case it is polynomial). Default=1.\n",
    "\n",
    "smoothing_fwhm= 6\n",
    "noise_model='ar1'\n",
    "\n",
    "# Confounds to use in GLM\n",
    "confounds2use = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "motion_names = confounds2use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Loading all files needed and organizing them\n",
    "Everything in these following chunks will be used for all types of modelling. If any modification is needed for a specific type of modelling, it will be done later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1- Events data - creating dictionary of events\n",
    "After setting the directories:\n",
    "- First, we will upload all csv files with all the events information. We have one for each run of each subject. \n",
    "- Then, we will create a dictionary that organizes all these events information. These informations came from the previous part of the code (where we organized all the behavioral csv files), but putting them into a dictionary make everything easier to access in loops later, as we can access them sorting by subject and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining directories\n",
    "data_dir = '/your_path/BIDS/'\n",
    "PRE_PROC_dir = '/your_path/fmriprep'\n",
    "\n",
    "## Events data\n",
    "# Get CSV files list from a folder\n",
    "behavioral_files = [y for x in os.walk(data_dir)\n",
    "                    for y in glob.glob(os.path.join(x[0], 'sub-*_run-0*_events.tsv'))]\n",
    "\n",
    "behavioral_files.sort()\n",
    "\n",
    "\n",
    "# Create dictionary of events per subject and run\n",
    "\n",
    "events_dict = {}\n",
    "\n",
    "for file in behavioral_files:\n",
    "     #Open events df with relevant columns for glm\n",
    "    df_new = pd.read_csv(file, sep='\\t', usecols=['onset','trial','duration','trial_type', 'subj', 'reward'])\n",
    "    # Rename reward column to clip\n",
    "    df_new = df_new.rename(columns={\"reward\": \"clip\"})\n",
    "    #slipt and keep only clip number\n",
    "    df_new[\"clip\"] = df_new['clip'].str.split('/').str[1]\n",
    "    # add to dictionary key with subj and run\n",
    "    events_dict[(file.split('/')[6]),(file.split('_')[-2])] = df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Creating dictionaries - Functional, mask and confounds files\n",
    "No mistery here, just uploading the files and creating simple dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get functional files list from a folder\n",
    "func_files = [y for x in os.walk(PRE_PROC_dir)\n",
    "                    for y in glob.glob(os.path.join(x[0], 'sub-*_task-amid_run-0*_desc-preproc_bold.nii.gz'))]\n",
    "func_files.sort()\n",
    "\n",
    "# create func dictionary\n",
    "func_files_dict = {}\n",
    "\n",
    "for file in func_files:\n",
    "    # add to dictionary key with subj and run\n",
    "    func_files_dict[(file.split('/')[7]),(file.split('_')[-4])] = file\n",
    "\n",
    "    \n",
    "    \n",
    "## Get mask files list for GLM\n",
    "mask_files = [y for x in os.walk(PRE_PROC_dir)\n",
    "                    for y in glob.glob(os.path.join(x[0], 'sub-*_run-0*brain_mask.nii.gz'))]\n",
    "mask_files.sort()\n",
    "\n",
    "# create mask dictionary\n",
    "mask_files_dict = {}\n",
    "for file in mask_files:\n",
    "    # add to dictionary key with subj and run\n",
    "    mask_files_dict[(file.split('/')[7]),(file.split('_')[-4])] = file\n",
    "    \n",
    "\n",
    "    \n",
    "## Get confounds files\n",
    "confounds_files = [y for x in os.walk(PRE_PROC_dir)\n",
    "                    for y in glob.glob(os.path.join(x[0], '*confounds_regressors.tsv'))]\n",
    "confounds_files.sort()\n",
    "\n",
    "# create dictionary for confounds per subject and run\n",
    "confounds_dict = {}\n",
    "\n",
    "for file in confounds_files:\n",
    "    # Open confounds df with relevant columns for glm\n",
    "    df_new = pd.read_csv(file, sep='\\t', usecols= confounds2use)\n",
    "    # add to dictionary key with subj and run\n",
    "    confounds_dict[(file.split('/')[7]),(file.split('_')[-3])] = df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Performing LSA Extraction\n",
    "- Creating LSA specific folders;\n",
    "- Transforming the original dataframe to the format expected to perform LSA;\n",
    "- Creating design matrices;\n",
    "- Fitting the model and saving results (betamaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outpath_lsa = '/your_path/GLM_LSA/'\n",
    "\n",
    "## Creating LSA folders\n",
    "for events in events_dict:\n",
    "    #if events == ('sub-XXXX'): # SKIP PROBLEMATIC SUBJECTS\n",
    "    #    continue    # continue here\n",
    "    # Create directories for each subjects if doesn't exist\n",
    "    if not os.path.exists('%s_LSA' % (events[0])):\n",
    "        outdir = outpath_lsa+'%s_LSA' % events[0];\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir);\n",
    "    outdir = outpath_lsa+'%s_LSA' % events[0]\n",
    "    \n",
    "    ## Transforming the DataFrame for LSA\n",
    "    lsa_events_df = events_dict[events].copy()\n",
    "    conditions = lsa_events_df['trial_type'].unique()\n",
    "    condition_counter = {c: 0 for c in conditions}\n",
    "    \n",
    "    for i_trial, trial in lsa_events_df.iterrows():\n",
    "        trial_condition = trial['trial_type']\n",
    "        condition_counter[trial_condition] += 1\n",
    "        run='run_'+events[1][-2:]\n",
    "        # We use a unique delimiter here (``_``) that shouldn't be in the\n",
    "        # original condition names\n",
    "        trial_name = f'{run}_{trial_condition}_{condition_counter[trial_condition]:02d}'\n",
    "        lsa_events_df.loc[i_trial, 'trial_type'] = trial_name\n",
    "        # save events csv for control\n",
    "        lsa_events_df.to_csv(outdir+\"/\"+events[0]+\"_\"+'run_'+events[1][-2:]+'_LSA_events.csv')\n",
    "    \n",
    "    \n",
    "\n",
    "    ## Creating design matrices\n",
    "    if not os.path.exists(outdir+\"/\"+events[0]+\"_\"+'run_'+events[1][-2:]+'_designmat.png'):\n",
    "        func_img = nb.load(func_files_dict[events])\n",
    "        n_scans = func_img.shape[-1]  # of volumes/scans per session\n",
    "        frame_times = np.arange(n_scans) * t_r  # here are the corresponding frame times\n",
    "        design_matrix = make_first_level_design_matrix(frame_times=frame_times,\n",
    "                                                         events=lsa_events_df, \n",
    "                                                         drift_model=drift_model, \n",
    "                                                         high_pass=high_pass,\n",
    "                                                         add_regs=confounds_dict[events], \n",
    "                                                         add_reg_names=motion_names,\n",
    "                                                         hrf_model=hrf_model)\n",
    "        # Save design matrices for control\n",
    "        plot_design_matrix(design_matrix, output_file = outdir+\"/\"+events[0]+\"_\"+'run_'+events[1][-2:]+'_designmat.png')\n",
    "\n",
    "        \n",
    "        ## fitting model        \n",
    "        lsa_glm = FirstLevelModel(mask_img=mask_files_dict[events],\n",
    "                                  t_r= t_r,\n",
    "                                  slice_time_ref=slice_time_ref,\n",
    "                                  hrf_model=hrf_model,\n",
    "                                  drift_model=drift_model,\n",
    "                                  high_pass=high_pass,\n",
    "                                standardize=True,\n",
    "                              smoothing_fwhm=smoothing_fwhm,\n",
    "                              noise_model=noise_model,\n",
    "                              #n_jobs=1,    \n",
    "                              minimize_memory=True,\n",
    "                              verbose=True,\n",
    "                              memory_level = 0)\n",
    "\n",
    "        lsa_glm.fit(run_imgs = func_files_dict[events], design_matrices = design_matrix)\n",
    "        \n",
    "        lsa_beta_maps = {cond: [] for cond in events_dict[events]['trial_type'].unique()}\n",
    "        trialwise_conditions = lsa_events_df['trial_type'].unique()\n",
    "\n",
    "        for condition in trialwise_conditions:\n",
    "            beta_map = lsa_glm.compute_contrast(condition, output_type='effect_size')\n",
    "            beta_map.to_filename(outdir+\"/\"+events[0]+\"_\"+'run_'+events[1][-2:]+\"_\"+condition[+7:]+'_beta.nii.gz')\n",
    "            #compute_contrast method returns the “unmasked” results. Mask the results and save\n",
    "            #beta_map_masked = math_img(\"img1 * img2\", img1=beta_map, img2=mask_files_dict[events])\n",
    "            #beta_map_masked.to_filename(outdir+\"/\"+events[0]+\"_\"+'run_'+events[1][-2:]+\"_\"+condition[+7:]+'_beta.nii.gz')\n",
    "            # Drop the trial number from the condition name to get the original name\n",
    "            condition_name = condition.split('_')[2]\n",
    "            lsa_beta_maps[condition_name].append(beta_map)\n",
    "\n",
    "        # We can concatenate the lists of 3D maps into a single 4D beta series for\n",
    "        # each condition, if we want\n",
    "        lsa_beta_maps = {name: image.concat_imgs(maps) for name, maps in lsa_beta_maps.items()}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
